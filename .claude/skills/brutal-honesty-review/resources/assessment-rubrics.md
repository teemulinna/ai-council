# Assessment Rubrics for Brutal Honesty Reviews

## Code Quality Rubric (Linus Mode)

### Correctness
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Failing** | Wrong algorithm, logic errors, crashes | `null` pointer dereference, off-by-one errors |
| ðŸŸ¡ **Passing** | Works in tested cases, no obvious bugs | Handles expected inputs correctly |
| ðŸŸ¢ **Excellent** | Proven correct across edge cases | Property-based tests, formal verification |

### Performance
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Failing** | Naive O(nÂ²) where O(n) exists | Nested loops for searchable data |
| ðŸŸ¡ **Passing** | Acceptable complexity for scale | O(n log n) for reasonable n |
| ðŸŸ¢ **Excellent** | Optimal algorithm + profiled | Cached, indexed, benchmarked |

### Error Handling
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Failing** | Crashes on invalid input | Uncaught exceptions, panics |
| ðŸŸ¡ **Passing** | Returns error codes/exceptions | `try/catch`, error returns |
| ðŸŸ¢ **Excellent** | Graceful degradation + logging | Circuit breakers, retry logic |

### Concurrency Safety
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Failing** | Race conditions, deadlocks | Shared mutable state, no locks |
| ðŸŸ¡ **Passing** | Thread-safe with locks | Proper mutex usage |
| ðŸŸ¢ **Excellent** | Lock-free or proven safe | Immutable data, atomic operations |

### Testability
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Failing** | Impossible to unit test | Hard-coded dependencies, global state |
| ðŸŸ¡ **Passing** | Can be tested with mocks | Dependency injection |
| ðŸŸ¢ **Excellent** | Self-testing design | Pure functions, property-based |

### Maintainability
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Failing** | "Clever" code, unclear intent | Obfuscated logic, magic numbers |
| ðŸŸ¡ **Passing** | Clear intent, reasonable | Named variables, comments |
| ðŸŸ¢ **Excellent** | Self-documenting + simple | Obvious code, minimal complexity |

**Passing Threshold**: Minimum ðŸŸ¡ on ALL criteria
**Ship-Ready**: Minimum ðŸŸ¢ on Correctness, Performance, Error Handling

---

## Test Quality Rubric (Ramsay Mode)

### Coverage
| Level | Criteria | Acceptable % |
|-------|----------|--------------|
| ðŸ”´ **Raw** | Only happy path | <50% branch |
| ðŸŸ¡ **Acceptable** | Common failures covered | 80%+ branch |
| ðŸŸ¢ **Michelin Star** | Complete boundary analysis | 95%+ branch + mutation tested |

### Edge Case Testing
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Raw** | Only happy path tested | `test('adds 2+2')` |
| ðŸŸ¡ **Acceptable** | Common failures tested | Null, empty, invalid input |
| ðŸŸ¢ **Michelin Star** | Boundary analysis complete | Min/max values, overflow, underflow |

### Test Clarity
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Raw** | Unclear what's being tested | `test('test1')` |
| ðŸŸ¡ **Acceptable** | Clear test names | `test('handles null input')` |
| ðŸŸ¢ **Michelin Star** | Self-documenting test pyramid | Given-When-Then, BDD style |

### Speed
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Raw** | Minutes to run unit tests | Calls real database/network |
| ðŸŸ¡ **Acceptable** | <10s for unit tests | Mocked dependencies |
| ðŸŸ¢ **Michelin Star** | <1s, parallelized | Pure functions, in-memory |

### Stability
| Level | Criteria | Flake Rate |
|-------|----------|------------|
| ðŸ”´ **Raw** | Flaky, timing-dependent | >1% failure rate |
| ðŸŸ¡ **Acceptable** | Stable but potentially slow | 0% flake, deterministic |
| ðŸŸ¢ **Michelin Star** | Deterministic + fast | 0% flake, <100ms per test |

### Isolation
| Level | Criteria | Example |
|-------|----------|---------|
| ðŸ”´ **Raw** | Tests depend on each other | Shared state, execution order matters |
| ðŸŸ¡ **Acceptable** | Independent tests | Each test sets up own state |
| ðŸŸ¢ **Michelin Star** | Pure functions, no shared state | Immutable, stateless |

**Merge Threshold**: Minimum ðŸŸ¡ on ALL criteria
**Production-Ready**: Minimum ðŸŸ¢ on Coverage, Stability, Isolation

---

## BS Detection Rubric (Bach Mode)

### Red Flags in Testing Practices

| Red Flag | Evidence | Impact | Harshness Level |
|----------|----------|--------|-----------------|
| **Cargo Cult Practice** | "Best practice" with no context | Wasted effort, false confidence | ðŸŸ¡ Harsh |
| **Certification Theater** | Required cert unrelated to actual skills | Filters out critical thinkers | ðŸŸ¢ Brutal |
| **Vendor Lock-In** | Tool solves problem it created | Expensive dependency | ðŸŸ¡ Harsh |
| **False Automation** | "AI testing" still needs human verification | Automation debt | ðŸŸ¡ Harsh |
| **Checkbox Quality** | Compliance without outcome measurement | Audit passes, customers suffer | ðŸŸ¢ Brutal |
| **Hype Cycle** | Promises 10x improvement without evidence | Budget waste, disillusionment | ðŸŸ¡ Harsh |
| **Coverage Theater** | 100% coverage of trivial code | False sense of quality | ðŸŸ¡ Harsh |
| **Test Script Slavery** | Following test cases without thinking | Misses actual bugs | ðŸŸ¢ Brutal |
| **Magic Tool Thinking** | Tool will solve all problems | Dependency without skill growth | ðŸŸ¡ Harsh |
| **Certification Over Competence** | Hiring based on credentials, not ability | Weak team, strong resumes | ðŸŸ¢ Brutal |

### Green Flag Test

Ask these questions about any practice/tool/certification:

1. **Does this help testers/developers do better work in THIS context?**
   - If yes â†’ Worth considering
   - If no â†’ BS alert

2. **Who benefits economically from this?**
   - Vendor/Consultant more than users â†’ BS alert
   - Users demonstrably benefit â†’ Potentially useful

3. **Can you measure the impact?**
   - Measurable outcomes â†’ Worth evaluating
   - Vague claims â†’ BS alert

4. **Does this promote thinking or compliance?**
   - Critical thinking â†’ Good
   - Checkbox compliance â†’ BS alert

5. **What happens if you don't adopt this?**
   - Concrete negative consequence â†’ Worth considering
   - FOMO, vendor says so â†’ BS alert

---

## Calibration Matrix

### When to Be Brutal

| Scenario | Linus | Ramsay | Bach | Notes |
|----------|-------|--------|------|-------|
| **Senior engineer, repeated mistake** | âœ… | âœ… | âœ… | They should know better |
| **Critical security bug** | âœ… | âœ… | âŒ | Technical precision needed |
| **Production incident** | âœ… | âœ… | âŒ | No time for sugar-coating |
| **Vendor evaluating claims** | âŒ | âŒ | âœ… | BS detection prevents waste |
| **Team explicitly requests no-BS** | âœ… | âœ… | âœ… | Permission granted |
| **Certification/process evaluation** | âŒ | âŒ | âœ… | Bach's specialty |

### When to Dial Back

| Scenario | Instead Use | Reason |
|----------|-------------|--------|
| **Junior dev, first PR** | Constructive mentoring | Build confidence |
| **Demoralized team** | Supportive guidance | Rebuild trust |
| **Public forum** | Private feedback | Avoid humiliation |
| **Unclear if fixable** | Collaborative problem-solving | Avoid frustration |
| **Personal, not technical** | Empathy + support | Not a code issue |

---

## Scoring Guide

### Overall Code Review Score (Linus Mode)

```
Score = (Correctness Ã— 3) + (Performance Ã— 2) + (Error Handling Ã— 3) +
        (Concurrency Ã— 2) + (Testability Ã— 1) + (Maintainability Ã— 1)

Maximum: 60 points (all Excellent)
Passing: 36 points (all Passing)
Failing: <36 points

Harshness Level:
- 0-24 points: ðŸ”´ Brutal ("This is fundamentally broken")
- 25-35 points: ðŸŸ¡ Harsh ("Multiple issues need addressing")
- 36-48 points: ðŸŸ¢ Direct ("Some improvements needed")
- 49-60 points: âšª Professional ("Minor suggestions")
```

### Test Suite Score (Ramsay Mode)

```
Score = (Coverage Ã— 3) + (Edge Cases Ã— 3) + (Clarity Ã— 1) +
        (Speed Ã— 1) + (Stability Ã— 3) + (Isolation Ã— 1)

Maximum: 60 points (all Michelin Star)
Merge Threshold: 36 points (all Acceptable)
Failing: <36 points

Harshness Level:
- 0-24 points: ðŸ”´ Brutal ("This is RAW. Don't serve it.")
- 25-35 points: ðŸŸ¡ Harsh ("You know what good looks like.")
- 36-48 points: ðŸŸ¢ Direct ("Close, but needs refinement.")
- 49-60 points: âšª Professional ("Well done, minor polish.")
```

### BS Detection Score (Bach Mode)

```
Red Flags: Count from BS Detection Rubric
Green Flags: Passes all 5 Green Flag Tests

Score = (Green Flags Ã— 20) - (Red Flags Ã— 10)

Maximum: 100 (all green flags, no red flags)
Acceptable: 50+ (more green than red)
BS Alert: <50 (more red than green)

Harshness Level:
- Negative score: ðŸ”´ Brutal ("This is harmful")
- 0-40: ðŸŸ¡ Harsh ("This is questionable")
- 41-70: ðŸŸ¢ Direct ("Some concerns")
- 71-100: âšª Professional ("Reasonable approach")
```

---

## Example Assessments

### Code Review Example (Linus Mode)

**Code**: Database query in HTTP handler without connection pooling

**Assessment**:
- Correctness: ðŸ”´ Failing (connection leak)
- Performance: ðŸ”´ Failing (O(n) connections)
- Error Handling: ðŸŸ¡ Passing (has try/catch)
- Concurrency: ðŸ”´ Failing (connection exhaustion)
- Testability: ðŸŸ¡ Passing (can mock)
- Maintainability: ðŸŸ¡ Passing (clear intent)

**Score**: (0Ã—3) + (0Ã—2) + (1Ã—3) + (0Ã—2) + (1Ã—1) + (1Ã—1) = 5/60

**Harshness**: ðŸ”´ Brutal

**Feedback**:
> "This is fundamentally broken. You're creating a new database connection
> for every HTTP request without pooling. Under load, you'll exhaust
> connections in seconds. Did you even test this with concurrent users?
>
> Use a connection pool. This is Database 101."

---

### Test Suite Example (Ramsay Mode)

**Tests**: 15 tests, all happy path, no edge cases

**Assessment**:
- Coverage: ðŸ”´ Raw (35% branch)
- Edge Cases: ðŸ”´ Raw (none tested)
- Clarity: ðŸŸ¡ Acceptable (clear names)
- Speed: ðŸŸ¢ Michelin (fast)
- Stability: ðŸŸ¢ Michelin (no flakes)
- Isolation: ðŸŸ¡ Acceptable (independent)

**Score**: (0Ã—3) + (0Ã—3) + (1Ã—1) + (2Ã—1) + (2Ã—3) + (1Ã—1) = 9/60

**Harshness**: ðŸ”´ Brutal

**Feedback**:
> "Look at this. You've got 15 tests and 14 of them are happy path.
> Where's the validation testing? Where's the error handling?
>
> This is RAW. You're testing if the code runs, not if it's correct.
> Don't merge this."

---

### BS Detection Example (Bach Mode)

**Claim**: "Our AI-powered test automation eliminates manual testing"

**Assessment**:
- Red Flags: Hype Cycle, Magic Tool Thinking, False Automation
- Green Flags: 0/5 (fails all tests)

**Score**: (0Ã—20) - (3Ã—10) = -30/100

**Harshness**: ðŸ”´ Brutal

**Feedback**:
> "This is vendor hype. 'AI-powered' doesn't eliminate the need for humans
> to define test oracles, handle edge cases, or investigate failures.
>
> The real question: Does this tool help YOUR testers on YOUR product
> in YOUR context? If you can't answer specifically, you're buying hype."
